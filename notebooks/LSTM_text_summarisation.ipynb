{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthikeyan37/Deep-Reinforcement-Learning/blob/main/notebooks/LSTM_text_summarisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "fYN4PvY-kBmB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d gowrishankarp/newspaper-text-summarization-cnn-dailymail\n",
        "!unzip newspaper-text-summarization-cnn-dailymail.zip\n"
      ],
      "metadata": {
        "id": "hgBAgMmXl0wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, AdditiveAttention\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Rw2oC1oqmcXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = pd.read_csv('cnn_dailymail/train.csv')\n",
        "dataset_train=dataset_train.drop(['id'], axis=1)\n",
        "dataset_train.head()"
      ],
      "metadata": {
        "id": "En1XezW3l0yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_validation=pd.read_csv('cnn_dailymail/validation.csv')\n",
        "dataset_validation=dataset_validation.drop(['id'], axis=1)\n",
        "dataset_validation.head()"
      ],
      "metadata": {
        "id": "seGBi65_l01I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test=pd.read_csv('cnn_dailymail/test.csv')\n",
        "dataset_test=dataset_test.drop(['id'], axis=1)\n",
        "dataset_test.head()"
      ],
      "metadata": {
        "id": "BLkheDLHl03w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#taking 10% sample of the train, validation, and test sets\n",
        "train_sampled = dataset_train.sample(frac=0.1, random_state=42) # Use sample instead of shuffle and select\n",
        "val_sampled = dataset_validation.sample(frac=0.1, random_state=42) # Use sample instead of shuffle and select\n",
        "test_sampled = dataset_test.sample(frac=0.1, random_state=42) # Use sample instead of shuffle and select\n",
        "\n",
        "\n",
        "print(f\"Train size: {len(train_sampled)}\")\n",
        "print(f\"Validation size: {len(val_sampled)}\")\n",
        "print(f\"Test size: {len(test_sampled)}\")"
      ],
      "metadata": {
        "id": "rcAputOzl06O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning text function\n",
        "def clean_text(text):\n",
        "    text = text.lower()  #lowercasing\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)  #removing special characters\n",
        "    return text"
      ],
      "metadata": {
        "id": "kkKKAavDl08w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_articles = [clean_text(i) for i in train_sampled['article']]\n"
      ],
      "metadata": {
        "id": "T3ccqOT3mG35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_index = max(range(len(cleaned_articles)), key=lambda i: len(cleaned_articles[i].split()))\n",
        "print(f\"Index of the article with the max length: {max_len_index}\")\n"
      ],
      "metadata": {
        "id": "We7tpZKSmG6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract articles and highlights for each split\n",
        "train_articles = train_sampled['article'].values\n",
        "train_highlights = train_sampled['highlights'].values\n",
        "\n",
        "val_articles = val_sampled['article'].values\n",
        "val_highlights = val_sampled['highlights'].values\n",
        "\n",
        "test_articles = test_sampled['article'].values\n",
        "test_highlights = test_sampled['highlights'].values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PG6syXi1kBpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set tokenizer parameters\n",
        "vocab_size = 50000\n",
        "max_article_len = 500\n",
        "max_summary_len = 50\n",
        "\n",
        "# Initialize and fit tokenizers on training data only\n",
        "article_tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<UNK>\")\n",
        "summary_tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<UNK>\")\n",
        "\n",
        "article_tokenizer.fit_on_texts(train_articles)\n",
        "summary_tokenizer.fit_on_texts(train_highlights)"
      ],
      "metadata": {
        "id": "tmNqVyGmkBrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to tokenize and pad sequences\n",
        "def tokenize_and_pad(texts, tokenizer, max_len):\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "    return padded"
      ],
      "metadata": {
        "id": "RV0UEcyakBum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and pad each dataset split\n",
        "train_article_padded = tokenize_and_pad(train_articles, article_tokenizer, max_article_len)\n",
        "train_summary_padded = tokenize_and_pad(train_highlights, summary_tokenizer, max_summary_len)\n",
        "\n",
        "val_article_padded = tokenize_and_pad(val_articles, article_tokenizer, max_article_len)\n",
        "val_summary_padded = tokenize_and_pad(val_highlights, summary_tokenizer, max_summary_len)\n",
        "\n",
        "test_article_padded = tokenize_and_pad(test_articles, article_tokenizer, max_article_len)\n",
        "test_summary_padded = tokenize_and_pad(test_highlights, summary_tokenizer, max_summary_len)"
      ],
      "metadata": {
        "id": "3uivcIWvkBxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare decoder input and output for training\n",
        "train_decoder_input = train_summary_padded[:, :-1]\n",
        "train_decoder_output = train_summary_padded[:, 1:]\n",
        "\n",
        "val_decoder_input = val_summary_padded[:, :-1]\n",
        "val_decoder_output = val_summary_padded[:, 1:]"
      ],
      "metadata": {
        "id": "02s2L5wRkB0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the encoder-decoder model with attention\n",
        "embedding_dim = 256\n",
        "lstm_units = 512\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_article_len,))\n",
        "enc_emb = Embedding(vocab_size, embedding_dim, trainable=True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "aH2z7sGkkB29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention Layer\n",
        "attention = AdditiveAttention()"
      ],
      "metadata": {
        "id": "zlcXGKq4kB5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_summary_len-1,))\n",
        "dec_emb_layer = Embedding(vocab_size, embedding_dim, trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)"
      ],
      "metadata": {
        "id": "pbxt2UbFkB8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply attention between encoder and decoder\n",
        "context_vector = attention([encoder_outputs, decoder_outputs])\n",
        "decoder_combined_context = tf.concat([context_vector, decoder_outputs], axis=-1)\n",
        "\n",
        "# Dense layer for predictions\n",
        "dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = dense(decoder_combined_context)\n",
        "\n",
        "# Define the final model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "rCEn6pM4kB_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using training and validation sets\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "\n",
        "history = model.fit(\n",
        "    [train_article_padded, train_decoder_input],\n",
        "    train_decoder_output,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=([val_article_padded, val_decoder_input], val_decoder_output)\n",
        ")\n"
      ],
      "metadata": {
        "id": "3TKUvfm7kCCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate([test_article_padded, test_summary_padded[:, :-1]], test_summary_padded[:, 1:])\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "PCMCsklPkCFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define inference models\n",
        "# Encoder model\n",
        "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder model for inference\n",
        "decoder_state_input_h = Input(shape=(lstm_units,))\n",
        "decoder_state_input_c = Input(shape=(lstm_units,))\n",
        "decoder_hidden_state_input = Input(shape=(max_article_len, lstm_units))\n",
        "\n",
        "dec_emb_inf = dec_emb_layer(decoder_inputs)\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(\n",
        "    dec_emb_inf, initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
        ")"
      ],
      "metadata": {
        "id": "BtNLQf43kCIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply attention for inference\n",
        "context_vector_inf = attention([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_combined_context2 = tf.concat([context_vector_inf, decoder_outputs2], axis=-1)\n",
        "decoder_outputs2 = dense(decoder_combined_context2)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2]\n",
        ")"
      ],
      "metadata": {
        "id": "RcKv9Dx3kCK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate summary using the trained model\n",
        "def decode_sequence(input_seq):\n",
        "    enc_outs, h, c = encoder_model.predict(input_seq)\n",
        "    target_seq = np.array([summary_tokenizer.word_index['<start>']])\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [enc_outs, h, c])\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = summary_tokenizer.index_word[sampled_token_index]\n",
        "\n",
        "        if sampled_word == '<end>' or len(decoded_sentence.split()) > max_summary_len:\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        target_seq = np.array([sampled_token_index])\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "dKLC_ur9kCN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage to generate summaries\n",
        "for i in range(5):\n",
        "    input_seq = test_article_padded[i:i+1]\n",
        "    summary = decode_sequence(input_seq)\n",
        "    print(\"Original:\", test_articles[i])\n",
        "    print(\"Generated Summary:\", summary)\n",
        "    print(\"Actual Summary:\", test_highlights[i])\n",
        "    print(\"-----\")\n"
      ],
      "metadata": {
        "id": "8Z09ioiWkCQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fiMwY6G3kCUB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Text summarization pre-processing and training.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}